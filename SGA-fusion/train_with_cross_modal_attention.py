#!/usr/bin/env python
"""
å®Œæ•´è®­ç»ƒè„šæœ¬ - ä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›çš„ CrysMMNet

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•è®­ç»ƒé›†æˆè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶çš„ CrysMMNet æ¨¡å‹ã€‚
æ”¯æŒ JARVIS-DFT å’Œ Material Project æ•°æ®é›†ã€‚

ç”¨æ³•ç¤ºä¾‹:
    # JARVIS æ•°æ®é›†è®­ç»ƒ
    python train_with_cross_modal_attention.py \
        --dataset jarvis \
        --property formation_energy \
        --use_cross_modal True \
        --num_heads 4

    # Material Project æ•°æ®é›†è®­ç»ƒ
    python train_with_cross_modal_attention.py \
        --dataset mp \
        --property band_gap \
        --use_cross_modal True \
        --num_heads 8
"""

import os
import sys
import csv
import time
import json
import argparse
import numpy as np
import pickle as pkl
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from jarvis.core.atoms import Atoms
from jarvis.db.jsonutils import loadjson, dumpjson

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'crysmmnet-main/src'))

from data import get_train_val_loaders
from train import train_dgl
from config import TrainingConfig
from models.alignn import ALIGNN, ALIGNNConfig

from transformers import AutoTokenizer, AutoModel
from tokenizers.normalizers import BertNormalizer


# ==================== è¾…åŠ©å‡½æ•° ====================

def str2bool(v):
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¸ƒå°”å€¼ï¼ˆç”¨äºargparseï¼‰"""
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('å¸ƒå°”å€¼åº”ä¸º yes/no, true/false, t/f, y/n, 1/0')


# ==================== é…ç½®å‚æ•° ====================

def get_parser():
    """æ„å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='è®­ç»ƒ CrysMMNet (å¸¦è·¨æ¨¡æ€æ³¨æ„åŠ›)',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--root_dir', type=str, default='../dataset/',
                        help='æ•°æ®é›†æ ¹ç›®å½•ï¼ˆç›¸å¯¹äºå½“å‰ç›®å½•æˆ–ç»å¯¹è·¯å¾„ï¼‰')
    parser.add_argument('--dataset', type=str, default='jarvis',
                        choices=['jarvis', 'mp', 'class', 'toy'],
                        help='æ•°æ®é›†åç§°: jarvis, mp, class (åˆ†ç±»), toy')
    parser.add_argument('--property', type=str, default='formation_energy',
                        help='é¢„æµ‹çš„æ€§è´¨ (å›å½’: formation_energy, band_gap; åˆ†ç±»: syn, metal_oxideç­‰)')

    # é¢„å¤„ç†æ•°æ®å‚æ•°
    parser.add_argument('--use_preprocessed', type=str2bool, default=False,
                        help='æ˜¯å¦ä½¿ç”¨é¢„å¤„ç†çš„å›¾æ•°æ®ï¼ˆå¤§å¹…åŠ å¿«åŠ è½½é€Ÿåº¦ï¼‰')
    parser.add_argument('--preprocessed_dir', type=str, default='preprocessed_data',
                        help='é¢„å¤„ç†æ•°æ®ç›®å½•')

    # æ•°æ®åˆ’åˆ†å‚æ•°
    parser.add_argument('--train_ratio', type=float, default=0.8,
                        help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                        help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--test_ratio', type=float, default=0.1,
                        help='æµ‹è¯•é›†æ¯”ä¾‹')
    parser.add_argument('--n_train', type=int, default=None,
                        help='è®­ç»ƒæ ·æœ¬æ•°ï¼ˆå¦‚æœæŒ‡å®šåˆ™è¦†ç›–train_ratioï¼‰')
    parser.add_argument('--n_val', type=int, default=None,
                        help='éªŒè¯æ ·æœ¬æ•°')
    parser.add_argument('--n_test', type=int, default=None,
                        help='æµ‹è¯•æ ·æœ¬æ•°')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=64,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=1000,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                        help='æƒé‡è¡°å‡')
    parser.add_argument('--warmup_steps', type=int, default=2000,
                        help='å­¦ä¹ ç‡warmupæ­¥æ•°')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--alignn_layers', type=int, default=4,
                        help='ALIGNNå±‚æ•°')
    parser.add_argument('--gcn_layers', type=int, default=4,
                        help='GCNå±‚æ•°')
    parser.add_argument('--hidden_features', type=int, default=256,
                        help='éšè—å±‚ç‰¹å¾ç»´åº¦')
    parser.add_argument('--graph_dropout', type=float, default=0.0,
                        help='ALIGNN/GCNå±‚çš„dropoutç‡ï¼ˆç”¨äºæ­£åˆ™åŒ–ï¼‰')

    # è·¨æ¨¡æ€æ³¨æ„åŠ›å‚æ•°ï¼ˆæ™šæœŸèåˆï¼‰
    parser.add_argument('--use_cross_modal', type=str2bool, default=True,
                        help='æ˜¯å¦ä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆæ™šæœŸèåˆï¼‰')
    parser.add_argument('--cross_modal_hidden_dim', type=int, default=256,
                        help='è·¨æ¨¡æ€æ³¨æ„åŠ›éšè—å±‚ç»´åº¦')
    parser.add_argument('--cross_modal_num_heads', type=int, default=4,
                        choices=[1, 2, 4, 8],
                        help='è·¨æ¨¡æ€æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--cross_modal_dropout', type=float, default=0.1,
                        help='è·¨æ¨¡æ€æ³¨æ„åŠ›dropoutç‡')

    # åæœŸèåˆæ–¹å¼å‚æ•° â­ NEW!
    parser.add_argument('--late_fusion_type', type=str, default='concat',
                        choices=['concat', 'gated', 'bilinear', 'adaptive', 'tucker'],
                        help='åæœŸèåˆæ–¹å¼: concat(æ‹¼æ¥), gated(é—¨æ§), bilinear(åŒçº¿æ€§), adaptive(è‡ªé€‚åº”), tucker(Tuckeråˆ†è§£)')
    parser.add_argument('--late_fusion_rank', type=int, default=16,
                        help='åŒçº¿æ€§/Tuckerèåˆçš„ç§©ï¼ˆä½ç§©åˆ†è§£å‚æ•°ï¼Œè¶Šå¤§è¡¨è¾¾èƒ½åŠ›è¶Šå¼ºä½†å‚æ•°è¶Šå¤šï¼‰')
    parser.add_argument('--late_fusion_output_dim', type=int, default=64,
                        help='åæœŸèåˆè¾“å‡ºç»´åº¦ï¼ˆå¯¹äºéconcatèåˆï¼‰')

    # ä¸­æœŸèåˆå‚æ•°
    parser.add_argument('--use_middle_fusion', type=str2bool, default=False,
                        help='æ˜¯å¦ä½¿ç”¨ä¸­æœŸèåˆï¼ˆåœ¨ç¼–ç è¿‡ç¨‹ä¸­æ³¨å…¥æ–‡æœ¬ä¿¡æ¯ï¼‰')
    parser.add_argument('--middle_fusion_layers', type=str, default='2',
                        help='ä¸­æœŸèåˆæ³¨å…¥çš„å±‚ç´¢å¼•ï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚ "2" æˆ– "2,3"ï¼‰')
    parser.add_argument('--middle_fusion_hidden_dim', type=int, default=128,
                        help='ä¸­æœŸèåˆéšè—å±‚ç»´åº¦')
    parser.add_argument('--middle_fusion_num_heads', type=int, default=2,
                        choices=[1, 2, 4],
                        help='ä¸­æœŸèåˆæ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--middle_fusion_dropout', type=float, default=0.1,
                        help='ä¸­æœŸèåˆdropoutç‡')
    parser.add_argument('--middle_fusion_use_gate_norm', type=str2bool, default=False,
                        help='ä¸­æœŸèåˆæ˜¯å¦ä½¿ç”¨Gate LayerNormï¼ˆç”¨äºç‰¹å¾å°ºåº¦å¹³è¡¡ï¼‰')
    parser.add_argument('--middle_fusion_use_learnable_scale', type=str2bool, default=False,
                        help='ä¸­æœŸèåˆæ˜¯å¦ä½¿ç”¨å¯å­¦ä¹ ç¼©æ”¾å› å­ï¼ˆè‡ªåŠ¨å­¦ä¹ æœ€ä¼˜æ–‡æœ¬ç‰¹å¾æƒé‡ï¼‰')
    parser.add_argument('--middle_fusion_initial_scale', type=float, default=1.0,
                        help='ä¸­æœŸèåˆå¯å­¦ä¹ ç¼©æ”¾å› å­çš„åˆå§‹å€¼ï¼ˆå»ºè®®åŸºäºè¯Šæ–­ç»“æœè®¾ç½®ï¼Œå¦‚12.0ï¼‰')

    # ç»†ç²’åº¦æ³¨æ„åŠ›å‚æ•°ï¼ˆåŸå­-æ–‡æœ¬tokençº§åˆ«ï¼‰â­ NEW!
    parser.add_argument('--use_fine_grained_attention', type=str2bool, default=False,
                        help='æ˜¯å¦ä½¿ç”¨ç»†ç²’åº¦æ³¨æ„åŠ›ï¼ˆåŸå­-æ–‡æœ¬tokençº§åˆ«ï¼‰')
    parser.add_argument('--fine_grained_hidden_dim', type=int, default=256,
                        help='ç»†ç²’åº¦æ³¨æ„åŠ›éšè—å±‚ç»´åº¦')
    parser.add_argument('--fine_grained_num_heads', type=int, default=8,
                        choices=[1, 2, 4, 8],
                        help='ç»†ç²’åº¦æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--fine_grained_dropout', type=float, default=0.1,
                        help='ç»†ç²’åº¦æ³¨æ„åŠ›dropoutç‡')
    parser.add_argument('--fine_grained_use_projection', type=str2bool, default=True,
                        help='ç»†ç²’åº¦æ³¨æ„åŠ›æ˜¯å¦ä½¿ç”¨æŠ•å½±å±‚')
    parser.add_argument('--mask_stopwords', type=int, default=0,
                        help='æ˜¯å¦åœ¨è®­ç»ƒæ—¶maskåœç”¨è¯ (0/1)')
    parser.add_argument('--remove_stopwords', type=int, default=0,
                        help='æ˜¯å¦åœ¨BERTç¼–ç å‰åˆ é™¤åœç”¨è¯ (0/1)')
    parser.add_argument('--stopwords_dir', type=str, default='',
                        help='åœç”¨è¯ç›®å½•è·¯å¾„ (é»˜è®¤: ./stopwords/en/)')

    # å¯¹æ¯”å­¦ä¹ å‚æ•°
    parser.add_argument('--use_contrastive', type=str2bool, default=False,
                        help='æ˜¯å¦ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±')
    parser.add_argument('--contrastive_weight', type=float, default=0.1,
                        help='å¯¹æ¯”å­¦ä¹ æŸå¤±çš„æƒé‡ï¼ˆç›¸å¯¹äºä¸»ä»»åŠ¡æŸå¤±ï¼‰')
    parser.add_argument('--contrastive_temperature', type=float, default=0.1,
                        help='å¯¹æ¯”å­¦ä¹ çš„æ¸©åº¦å‚æ•°')

    # èåˆç­–ç•¥å‚æ•° â­ NEW! (æ–¹æ¡ˆ1å’Œæ–¹æ¡ˆ2)
    parser.add_argument('--cross_modal_attention_type', type=str, default='bidirectional',
                        choices=['bidirectional', 'unidirectional'],
                        help='è·¨æ¨¡æ€æ³¨æ„åŠ›ç±»å‹: bidirectional (åŒå‘), unidirectional (å•å‘ï¼Œæ–‡æœ¬â†’å›¾)')
    parser.add_argument('--fusion_strategy', type=str, default='gated',
                        choices=['average', 'concat', 'gated'],
                        help='èåˆç­–ç•¥: average (å¹³å‡), concat (æ‹¼æ¥), gated (é—¨æ§)')
    parser.add_argument('--gated_fusion_type', type=str, default='dual_gate',
                        choices=['single_gate', 'dual_gate', 'attention'],
                        help='é—¨æ§èåˆç±»å‹: single_gate, dual_gate (æ¨è), attention')
    parser.add_argument('--gated_fusion_hidden_dim', type=int, default=128,
                        help='é—¨æ§èåˆéšè—å±‚ç»´åº¦')
    parser.add_argument('--gated_fusion_dropout', type=float, default=0.1,
                        help='é—¨æ§èåˆdropoutç‡')

    # Gated Cross-Attentionå‚æ•° â­ NEW! (æ–¹æ¡ˆä¸‰ - è´¨é‡æ„ŸçŸ¥è‡ªé€‚åº”èåˆ)
    # æ³¨æ„: è¿™äº›å‚æ•°æš‚æ—¶è¢«æ³¨é‡Šï¼Œå› ä¸º Gated Cross-Attention åŠŸèƒ½å°šæœªåœ¨ ALIGNNConfig ä¸­å®ç°
    # parser.add_argument('--use_gated_cross_attention', type=str2bool, default=False,
    #                     help='æ˜¯å¦ä½¿ç”¨Gated Cross-Attentionï¼ˆè´¨é‡æ„ŸçŸ¥è‡ªé€‚åº”èåˆï¼‰')
    # parser.add_argument('--gated_attention_hidden_dim', type=int, default=256,
    #                     help='Gated Cross-Attentionéšè—å±‚ç»´åº¦')
    # parser.add_argument('--gated_attention_num_heads', type=int, default=4,
    #                     choices=[1, 2, 4, 8],
    #                     help='Gated Cross-Attentionæ³¨æ„åŠ›å¤´æ•°')
    # parser.add_argument('--gated_attention_dropout', type=float, default=0.1,
    #                     help='Gated Cross-Attention dropoutç‡')
    # parser.add_argument('--gated_quality_hidden_dim', type=int, default=128,
    #                     help='è´¨é‡é—¨æ§ç½‘ç»œéšè—å±‚ç»´åº¦')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--output_dir', type=str, default='./output/',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--config_file', type=str, default=None,
                        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--resume', type=int, default=0,
                        help='æ˜¯å¦ä»checkpointæ¢å¤è®­ç»ƒ (0/1)')
    parser.add_argument('--random_seed', type=int, default=123,
                        help='éšæœºç§å­')
    parser.add_argument('--num_workers', type=int, default=0,
                        help='æ•°æ®åŠ è½½workersæ•°é‡')
    parser.add_argument('--early_stopping_patience', type=int, default=None,
                        help='Early stoppingè€å¿ƒå€¼ï¼ˆæ— æ”¹å–„çš„epochæ•°ï¼‰')

    # åˆ†ç±»ä»»åŠ¡å‚æ•°
    parser.add_argument('--classification', type=int, default=0,
                        help='æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡ (0/1)')
    parser.add_argument('--classification_threshold', type=float, default=0.5,
                        help='åˆ†ç±»é˜ˆå€¼ï¼ˆç”¨äºå¯ç”¨åˆ†ç±»æ¨¡å¼ï¼‰')

    return parser


# ==================== æ•°æ®é›†è·¯å¾„é…ç½® ====================

def get_dataset_paths(root_dir, dataset, property_name):
    """æ ¹æ®æ•°æ®é›†å’Œæ€§è´¨è·å–æ•°æ®è·¯å¾„"""

    if dataset.lower() == 'jarvis':
        # JARVIS-DFT æ•°æ®é›†
        property_map = {
            'formation_energy': 'formation_energy_peratom',
            'fe': 'formation_energy_peratom',
            'total_energy': 'optb88vdw_total_energy',
            'opt_bandgap': 'optb88vdw_bandgap',
            'mbj_bandgap': 'mbj_bandgap',
            'bulk_modulus': 'bulk_modulus_kv',
            'bulk_modulus_kv': 'bulk_modulus_kv',
            'shear_modulus': 'shear_modulus_gv',
            'shear_modulus_gv': 'shear_modulus_gv',
        }

        prop_folder = property_map.get(property_name, property_name)
        cif_dir = os.path.join(root_dir, f'jarvis/{prop_folder}/cif/')
        id_prop_file = os.path.join(root_dir, f'jarvis/{prop_folder}/description.csv')

    elif dataset.lower() == 'mp':
        # Material Project æ•°æ®é›†
        if property_name in ['formation_energy', 'band_gap']:
            cif_dir = os.path.join(root_dir, 'mp_2018_new/')
            id_prop_file = os.path.join(root_dir, 'mp_2018_new/mat_text.csv')
        elif property_name in ['bulk', 'shear', 'bulk_modulus', 'shear_modulus']:
            cif_dir = os.path.join(root_dir, 'mp_2018_small/cif/')
            id_prop_file = os.path.join(root_dir, 'mp_2018_small/description.csv')
        else:
            raise ValueError(f"Unsupported property for MP dataset: {property_name}")

    elif dataset.lower() == 'class':
        # åˆ†ç±»æ•°æ®é›†ï¼ˆç±»ä¼¼jarvisç»“æ„ï¼‰
        # ä¾‹å¦‚ï¼šclass/syn, class/metal_oxide, ç­‰
        cif_dir = os.path.join(root_dir, f'class/{property_name}/cif/')
        id_prop_file = os.path.join(root_dir, f'class/{property_name}/description.csv')

    elif dataset.lower() == 'toy':
        # ç©å…·æ•°æ®é›†ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        cif_dir = os.path.join(root_dir, 'toy/cif/')
        id_prop_file = os.path.join(root_dir, 'toy/description.csv')

    else:
        raise ValueError(f"Unsupported dataset: {dataset}")

    return cif_dir, id_prop_file


# ==================== æ•°æ®åŠ è½½ ====================

def load_preprocessed_dataset(preprocessed_dir, dataset, property_name):
    """åŠ è½½é¢„å¤„ç†çš„æ•°æ®é›†

    Args:
        preprocessed_dir: é¢„å¤„ç†æ•°æ®ç›®å½•
        dataset: æ•°æ®é›†åç§°
        property_name: å±æ€§åç§°

    Returns:
        train_data, val_data, test_data: ä¸‰ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (graph, line_graph, text, target)
    """
    import pickle

    print(f"\n{'='*60}")
    print(f"åŠ è½½é¢„å¤„ç†æ•°æ®é›†: {dataset} - {property_name}")
    print(f"é¢„å¤„ç†ç›®å½•: {preprocessed_dir}")
    print(f"{'='*60}\n")

    # åŠ è½½ä¸‰ä¸ªæ•°æ®é›†ï¼ˆä½¿ç”¨datasetåç§°ï¼Œä¸preprocess_graphs.pyä¸€è‡´ï¼‰
    splits = {}
    for split_name in ['train', 'val', 'test']:
        pkl_file = os.path.join(
            preprocessed_dir,
            f"{dataset.lower()}_{property_name}_{split_name}.pkl"
        )

        if not os.path.exists(pkl_file):
            raise FileNotFoundError(
                f"æ‰¾ä¸åˆ°é¢„å¤„ç†æ–‡ä»¶: {pkl_file}\n"
                f"è¯·å…ˆè¿è¡Œ: python preprocess_graphs.py --dataset {dataset} --property {property_name}"
            )

        print(f"åŠ è½½ {split_name} é›†: {pkl_file}")
        with open(pkl_file, 'rb') as f:
            samples = pickle.load(f)

        # è½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€çš„å­—å…¸æ ¼å¼ï¼ˆä¸load_datasetä¿æŒä¸€è‡´ï¼‰
        data = []
        for sample in samples:
            # ä»å›¾ä¸­æ¢å¤ atoms ä¿¡æ¯
            # æ³¨æ„ï¼šæˆ‘ä»¬ç›´æ¥ä½¿ç”¨é¢„æ„å»ºçš„å›¾ï¼Œä¸éœ€è¦ atoms.to_dict()
            info = {
                "graph": sample['graph'][0],      # atom graph (é¢„æ„å»º)
                "line_graph": sample['line_graph'],  # line graph (é¢„æ„å»º)
                "jid": sample['id'],
                "text": sample['text'],          # å·²è§„èŒƒåŒ–çš„æ–‡æœ¬
                "target": sample['target']
            }
            data.append(info)

        splits[split_name] = data
        print(f"  âœ“ åŠ è½½äº† {len(data)} ä¸ªæ ·æœ¬")

    print(f"\næˆåŠŸåŠ è½½é¢„å¤„ç†æ•°æ®!")
    print(f"  è®­ç»ƒé›†: {len(splits['train'])} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(splits['val'])} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(splits['test'])} æ ·æœ¬\n")

    return splits['train'], splits['val'], splits['test']


def load_dataset(cif_dir, id_prop_file, dataset, property_name):
    """åŠ è½½æ•°æ®é›†"""
    print(f"\n{'='*60}")
    print(f"åŠ è½½æ•°æ®é›†: {dataset} - {property_name}")
    print(f"CIFç›®å½•: {cif_dir}")
    print(f"æè¿°æ–‡ä»¶: {id_prop_file}")
    print(f"{'='*60}\n")

    # è¯»å–CSVæ–‡ä»¶
    with open(id_prop_file, 'r') as f:
        reader = csv.reader(f)
        headings = next(reader)
        data = [row for row in reader]

    print(f"æ€»æ ·æœ¬æ•°: {len(data)}")

    # æ–‡æœ¬å½’ä¸€åŒ–å™¨
    norm = BertNormalizer(lowercase=False, strip_accents=True,
                         clean_text=True, handle_chinese_chars=True)

    # åŠ è½½è¯æ±‡æ˜ å°„ - æ™ºèƒ½è·¯å¾„æŸ¥æ‰¾
    # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
    possible_paths = [
        'vocab_mappings.txt',  # å½“å‰ç›®å½•
        './vocab_mappings.txt',
        os.path.join(os.path.dirname(__file__), 'vocab_mappings.txt'),  # è„šæœ¬æ‰€åœ¨ç›®å½•
        os.path.join(os.path.dirname(__file__), 'crysmmnet-main/src/vocab_mappings.txt'),  # ä»æ ¹ç›®å½•
        '../vocab_mappings.txt',  # ä¸Šçº§ç›®å½•
        '../../vocab_mappings.txt',
    ]

    vocab_file = None
    for path in possible_paths:
        if os.path.exists(path):
            vocab_file = path
            break

    if vocab_file is None:
        raise FileNotFoundError(
            "æ— æ³•æ‰¾åˆ° vocab_mappings.txt æ–‡ä»¶ã€‚è¯·ç¡®ä¿ï¼š\n"
            "1. æ–‡ä»¶å­˜åœ¨äº crysmmnet-main/src/ ç›®å½•\n"
            "2. å½“å‰å·¥ä½œç›®å½•æ­£ç¡®\n"
            f"å°è¯•è¿‡çš„è·¯å¾„: {possible_paths}"
        )

    print(f"ä½¿ç”¨è¯æ±‡æ˜ å°„æ–‡ä»¶: {vocab_file}")
    with open(vocab_file, 'r') as f:
        mappings = f.read().strip().split('\n')
    mappings = {m[0]: m[2:] for m in mappings}

    def normalize(text):
        text = [norm.normalize_str(s) for s in text.split('\n')]
        out = []
        for s in text:
            norm_s = ''
            for c in s:
                norm_s += mappings.get(c, ' ')
            out.append(norm_s)
        return '\n'.join(out)

    # æ„å»ºæ•°æ®é›†
    dataset_array = []
    skipped = 0

    for j in tqdm(range(len(data)), desc="åŠ è½½æ•°æ®"):
        try:
            if dataset.lower() == 'mp':
                if property_name == 'formation_energy':
                    id, composition, target, _, crys_desc_full, _ = data[j]
                elif property_name == 'band_gap':
                    id, composition, _, target, crys_desc_full, _ = data[j]
                elif property_name == 'shear':
                    id, composition, target, _, crys_desc_full, _ = data[j]
                elif property_name in ['bulk', 'bulk_modulus']:
                    id, composition, _, target, crys_desc_full, _ = data[j]
            elif dataset.lower() == 'jarvis':
                id, composition, target, crys_desc_full, _ = data[j]
            elif dataset.lower() == 'class':
                # åˆ†ç±»æ•°æ®é›†æ ¼å¼ï¼šid, target, description
                id, target, crys_desc_full = data[j]
                composition = ''  # åˆ†ç±»ä»»åŠ¡ä¸éœ€è¦composition
            elif dataset.lower() == 'toy':
                id, composition, target, crys_desc_full, _ = data[j]

            # è¯»å–CIFæ–‡ä»¶
            file_path = os.path.join(cif_dir, f'{id}.cif')
            if not os.path.exists(file_path):
                skipped += 1
                continue

            atoms = Atoms.from_cif(file_path)

            # æ„å»ºæ ·æœ¬
            info = {
                "atoms": atoms.to_dict(),
                "jid": id,
                "text": crys_desc_full,
                "target": float(target)
            }

            # MPæ•°æ®é›†çš„ç‰¹æ®Šå¤„ç†
            if dataset.lower() == 'mp' and property_name in ['shear', 'bulk', 'bulk_modulus', 'shear_modulus']:
                info["target"] = np.log10(float(target))

            dataset_array.append(info)

        except Exception as e:
            skipped += 1
            if skipped <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                print(f"è·³è¿‡æ ·æœ¬ {id}: {e}")

    print(f"\næˆåŠŸåŠ è½½: {len(dataset_array)} æ ·æœ¬")
    print(f"è·³è¿‡: {skipped} æ ·æœ¬\n")

    return dataset_array


# ==================== é…ç½®ç”Ÿæˆ ====================

def create_config(args):
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºè®­ç»ƒé…ç½®"""

    # å¯¼å…¥ ALIGNNConfig
    from models.alignn import ALIGNNConfig

    # æ•°æ®é›†åç§°æ˜ å°„ï¼šç”¨æˆ·å‹å¥½åç§° -> TrainingConfig æœŸæœ›çš„åç§°
    dataset_mapping = {
        'jarvis': 'user_data',
        'mp': 'user_data',
        'class': 'user_data',
        'toy': 'user_data',
        'user_data': 'user_data',
    }

    # è·å–å®é™…çš„æ•°æ®é›†åç§°
    actual_dataset = dataset_mapping.get(args.dataset.lower(), 'user_data')

    # åˆ›å»ºæ¨¡å‹é…ç½®å¯¹è±¡
    model_config = ALIGNNConfig(
        name="alignn",
        alignn_layers=args.alignn_layers,
        gcn_layers=args.gcn_layers,
        atom_input_features=92,
        edge_input_features=80,
        triplet_input_features=40,
        embedding_features=64,
        hidden_features=args.hidden_features,
        output_features=1,
        # Graphå±‚ dropoutï¼ˆæ­£åˆ™åŒ–ï¼‰
        graph_dropout=args.graph_dropout,
        # è·¨æ¨¡æ€æ³¨æ„åŠ›é…ç½®ï¼ˆæ™šæœŸèåˆï¼‰
        use_cross_modal_attention=args.use_cross_modal,
        cross_modal_hidden_dim=args.cross_modal_hidden_dim,
        cross_modal_num_heads=args.cross_modal_num_heads,
        cross_modal_dropout=args.cross_modal_dropout,
        # åæœŸèåˆæ–¹å¼é…ç½® â­ NEW!
        late_fusion_type=args.late_fusion_type,
        late_fusion_rank=args.late_fusion_rank,
        late_fusion_output_dim=args.late_fusion_output_dim,
        # ä¸­æœŸèåˆé…ç½®
        use_middle_fusion=args.use_middle_fusion,
        middle_fusion_layers=args.middle_fusion_layers,
        middle_fusion_hidden_dim=args.middle_fusion_hidden_dim,
        middle_fusion_num_heads=args.middle_fusion_num_heads,
        middle_fusion_dropout=args.middle_fusion_dropout,
        middle_fusion_use_gate_norm=args.middle_fusion_use_gate_norm,
        middle_fusion_use_learnable_scale=args.middle_fusion_use_learnable_scale,
        middle_fusion_initial_scale=args.middle_fusion_initial_scale,
        # å¯¹æ¯”å­¦ä¹ é…ç½®
        use_contrastive_loss=args.use_contrastive,
        contrastive_loss_weight=args.contrastive_weight,
        contrastive_temperature=args.contrastive_temperature,
        # ç»†ç²’åº¦æ³¨æ„åŠ›é…ç½®ï¼ˆåŸå­-æ–‡æœ¬tokençº§åˆ«ï¼‰â­ NEW!
        use_fine_grained_attention=args.use_fine_grained_attention,
        fine_grained_hidden_dim=args.fine_grained_hidden_dim,
        fine_grained_num_heads=args.fine_grained_num_heads,
        fine_grained_dropout=args.fine_grained_dropout,
        fine_grained_use_projection=args.fine_grained_use_projection,
        mask_stopwords=bool(args.mask_stopwords),
        remove_stopwords=bool(args.remove_stopwords),
        stopwords_dir=args.stopwords_dir,
        link="identity",
        zero_inflated=False,
        classification=False
    )

    config = {
        "version": "cross_modal_attention_v1",
        "dataset": actual_dataset,  # ä½¿ç”¨æ˜ å°„åçš„åç§°
        "target": "target",
        "atom_features": "cgcnn",
        "neighbor_strategy": "k-nearest",
        "id_tag": "jid",
        "random_seed": args.random_seed,
        "classification_threshold": args.classification_threshold if args.classification else None,

        # æ•°æ®åˆ’åˆ†
        "n_train": args.n_train,
        "n_val": args.n_val,
        "n_test": args.n_test,
        "train_ratio": args.train_ratio if args.n_train is None else None,
        "val_ratio": args.val_ratio if args.n_val is None else None,
        "test_ratio": args.test_ratio if args.n_test is None else None,

        "target_multiplication_factor": None,

        # è®­ç»ƒå‚æ•°
        "epochs": args.epochs,
        "batch_size": args.batch_size,
        "weight_decay": args.weight_decay,
        "learning_rate": args.learning_rate,
        "filename": f"{args.dataset}_{args.property}",
        "warmup_steps": args.warmup_steps,
        "criterion": "bce" if args.classification else "mse",
        "optimizer": "adamw",
        "scheduler": "onecycle",

        "pin_memory": False,
        "save_dataloader": False,
        "write_checkpoint": True,
        "write_predictions": True,
        "store_outputs": True,
        "progress": True,
        "log_tensorboard": False,
        "standard_scalar_and_pca": False,
        "use_canonize": True,
        "num_workers": args.num_workers,
        "cutoff": 8.0,
        "max_neighbors": 12,
        "keep_data_order": False,
        "distributed": False,
        "n_early_stopping": args.early_stopping_patience,
        "output_dir": args.output_dir,

        # æ¨¡å‹é…ç½®å¯¹è±¡ï¼ˆè€Œä¸æ˜¯å­—å…¸ï¼‰
        "model": model_config
    }

    return config


# ==================== ä¸»è®­ç»ƒå‡½æ•° ====================

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""

    # è§£æå‚æ•°
    parser = get_parser()
    args = parser.parse_args()

    # æ‰“å°é…ç½®
    print("\n" + "="*80)
    print("CrysMMNet è®­ç»ƒ - è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶")
    print("="*80)
    print(f"\næ•°æ®é›†é…ç½®:")
    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  æ€§è´¨: {args.property}")
    print(f"  æ ¹ç›®å½•: {args.root_dir}")

    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"  å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"  æƒé‡è¡°å‡: {args.weight_decay}")
    print(f"  ä»»åŠ¡ç±»å‹: {'åˆ†ç±»' if args.classification else 'å›å½’'}")
    if args.classification:
        print(f"  åˆ†ç±»é˜ˆå€¼: {args.classification_threshold}")
        print(f"  æŸå¤±å‡½æ•°: BCE (Binary Cross Entropy)")

    print(f"\næ¨¡å‹é…ç½®:")
    print(f"  ALIGNNå±‚æ•°: {args.alignn_layers}")
    print(f"  GCNå±‚æ•°: {args.gcn_layers}")
    print(f"  éšè—å±‚ç»´åº¦: {args.hidden_features}")
    print(f"  Graphå±‚Dropout: {args.graph_dropout}")

    print(f"\nè·¨æ¨¡æ€æ³¨æ„åŠ›é…ç½®ï¼ˆæ™šæœŸèåˆï¼‰:")
    print(f"  å¯ç”¨: {args.use_cross_modal}")
    if args.use_cross_modal:
        print(f"  éšè—ç»´åº¦: {args.cross_modal_hidden_dim}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {args.cross_modal_num_heads}")
        print(f"  Dropoutç‡: {args.cross_modal_dropout}")

    print(f"\nåæœŸèåˆæ–¹å¼é…ç½®:")
    print(f"  èåˆç±»å‹: {args.late_fusion_type}")
    fusion_desc = {
        'concat': 'ç®€å•æ‹¼æ¥ï¼ˆåŸºçº¿ï¼‰',
        'gated': 'é—¨æ§èåˆï¼ˆè‡ªé€‚åº”æƒé‡ï¼‰',
        'bilinear': 'åŒçº¿æ€§æ± åŒ–ï¼ˆäºŒé˜¶äº¤äº’ï¼‰',
        'adaptive': 'è‡ªé€‚åº”èåˆï¼ˆå¤šç­–ç•¥ç»„åˆï¼‰',
        'tucker': 'Tuckeråˆ†è§£ï¼ˆé«˜é˜¶å¼ é‡ï¼‰'
    }
    print(f"  è¯´æ˜: {fusion_desc.get(args.late_fusion_type, 'æœªçŸ¥')}")
    if args.late_fusion_type in ['bilinear', 'tucker']:
        print(f"  ä½ç§©åˆ†è§£Rank: {args.late_fusion_rank}")
    if args.late_fusion_type != 'concat':
        print(f"  èåˆè¾“å‡ºç»´åº¦: {args.late_fusion_output_dim}")

    print(f"\nä¸­æœŸèåˆé…ç½®:")
    print(f"  å¯ç”¨: {args.use_middle_fusion}")
    if args.use_middle_fusion:
        print(f"  èåˆå±‚: {args.middle_fusion_layers}")
        print(f"  éšè—ç»´åº¦: {args.middle_fusion_hidden_dim}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {args.middle_fusion_num_heads}")
        print(f"  Dropoutç‡: {args.middle_fusion_dropout}")
        print(f"  Gate LayerNorm: {args.middle_fusion_use_gate_norm}")
        print(f"  å¯å­¦ä¹ ç¼©æ”¾: {args.middle_fusion_use_learnable_scale}")
        if args.middle_fusion_use_learnable_scale:
            print(f"  åˆå§‹ç¼©æ”¾å€¼: {args.middle_fusion_initial_scale}")

    print(f"\nç»†ç²’åº¦æ³¨æ„åŠ›é…ç½®ï¼ˆåŸå­-æ–‡æœ¬tokençº§åˆ«ï¼‰:")
    print(f"  å¯ç”¨: {args.use_fine_grained_attention}")
    if args.use_fine_grained_attention:
        print(f"  éšè—ç»´åº¦: {args.fine_grained_hidden_dim}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {args.fine_grained_num_heads}")
        print(f"  Dropoutç‡: {args.fine_grained_dropout}")
        print(f"  ä½¿ç”¨æŠ•å½±: {args.fine_grained_use_projection}")
        print(f"  åœç”¨è¯Masking: {bool(args.mask_stopwords)}")
        print(f"  åœç”¨è¯åˆ é™¤(BERTç¼–ç å‰): {bool(args.remove_stopwords)}")
        if args.mask_stopwords or args.remove_stopwords:
            print(f"  åœç”¨è¯ç›®å½•: {args.stopwords_dir if args.stopwords_dir else 'é»˜è®¤ (./stopwords/en/)'}")

    print(f"\nå¯¹æ¯”å­¦ä¹ é…ç½®:")
    print(f"  å¯ç”¨: {args.use_contrastive}")
    if args.use_contrastive:
        print(f"  æŸå¤±æƒé‡: {args.contrastive_weight}")
        print(f"  æ¸©åº¦å‚æ•°: {args.contrastive_temperature}")

    print(f"\nè¾“å‡ºç›®å½•: {args.output_dir}")
    print("="*80 + "\n")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join(args.output_dir, f"{args.property}/")
    os.makedirs(output_dir, exist_ok=True)

    # åŠ è½½æ•°æ®é›†
    if args.use_preprocessed:
        # ä½¿ç”¨é¢„å¤„ç†æ•°æ®ï¼ˆå¿«é€ŸåŠ è½½ï¼‰
        print(f"\nâš¡ ä½¿ç”¨é¢„å¤„ç†æ•°æ®åŠ è½½æ¨¡å¼")
        print(f"é¢„å¤„ç†ç›®å½•: {args.preprocessed_dir}\n")

        train_data, val_data, test_data = load_preprocessed_dataset(
            args.preprocessed_dir,
            args.dataset,
            args.property
        )
        dataset_array = (train_data, val_data, test_data)

    else:
        # ä»åŸå§‹ CIF æ–‡ä»¶åŠ è½½ï¼ˆæ…¢é€Ÿï¼‰
        print(f"\nğŸ¢ ä»åŸå§‹ CIF æ–‡ä»¶åŠ è½½æ¨¡å¼ï¼ˆè¾ƒæ…¢ï¼‰")
        print(f"æç¤º: ä½¿ç”¨ --use_preprocessed True å¯ä»¥å¤§å¹…åŠ å¿«åŠ è½½é€Ÿåº¦\n")

        # è·å–æ•°æ®è·¯å¾„
        cif_dir, id_prop_file = get_dataset_paths(args.root_dir, args.dataset, args.property)

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(cif_dir):
            print(f"\nâŒ é”™è¯¯: CIFç›®å½•ä¸å­˜åœ¨: {cif_dir}")
            print(f"\næç¤º:")
            print(f"  1. æ£€æŸ¥ --root_dir å‚æ•°æ˜¯å¦æ­£ç¡®")
            print(f"  2. å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
            print(f"  3. å¦‚æœåœ¨ src ç›®å½•ä¸‹è¿è¡Œï¼Œä½¿ç”¨: --root_dir ../dataset/")
            print(f"  4. å¦‚æœåœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼Œä½¿ç”¨: --root_dir ./crysmmnet-main/dataset/")
            raise FileNotFoundError(f"CIFç›®å½•ä¸å­˜åœ¨: {cif_dir}")
        if not os.path.exists(id_prop_file):
            print(f"\nâŒ é”™è¯¯: æè¿°æ–‡ä»¶ä¸å­˜åœ¨: {id_prop_file}")
            print(f"\næç¤º: è¯·ç¡®ä¿æ•°æ®é›†å·²æ­£ç¡®ä¸‹è½½å¹¶è§£å‹")
            raise FileNotFoundError(f"æè¿°æ–‡ä»¶ä¸å­˜åœ¨: {id_prop_file}")

        # åŠ è½½æ•°æ®é›†
        dataset_array = load_dataset(cif_dir, id_prop_file, args.dataset, args.property)

    # åˆ›å»ºé…ç½®
    config_dict = create_config(args)
    config_dict['output_dir'] = output_dir

    # æå–æ¨¡å‹é…ç½®ï¼ˆALIGNNConfigï¼‰ç”¨äºä¿å­˜åˆ°checkpoint
    model_config = config_dict['model']

    # ä¿å­˜é…ç½®ï¼ˆå°† ALIGNNConfig å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿ JSON åºåˆ—åŒ–ï¼‰
    config_file = os.path.join(output_dir, 'config.json')
    config_dict_serializable = config_dict.copy()

    # è½¬æ¢ model é…ç½®å¯¹è±¡ä¸ºå­—å…¸
    if hasattr(config_dict['model'], 'dict'):
        # Pydantic v1
        config_dict_serializable['model'] = config_dict['model'].dict()
    elif hasattr(config_dict['model'], 'model_dump'):
        # Pydantic v2
        config_dict_serializable['model'] = config_dict['model'].model_dump()
    else:
        # å°è¯•ä½¿ç”¨ __dict__
        config_dict_serializable['model'] = config_dict['model'].__dict__

    with open(config_file, 'w') as f:
        json.dump(config_dict_serializable, f, indent=4)
    print(f"é…ç½®å·²ä¿å­˜åˆ°: {config_file}\n")

    # è½¬æ¢ä¸ºTrainingConfigå¯¹è±¡
    try:
        config = TrainingConfig(**config_dict)
    except Exception as e:
        print(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
        return

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader, test_loader, prepare_batch = get_train_val_loaders(
        dataset_array=dataset_array,
        target=config.target,
        n_train=args.n_train,
        n_val=args.n_val,
        n_test=args.n_test,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        batch_size=config.batch_size,
        atom_features=config.atom_features,
        neighbor_strategy=config.neighbor_strategy,
        id_tag=config.id_tag,
        pin_memory=config.pin_memory,
        workers=config.num_workers,
        save_dataloader=config.save_dataloader,
        use_canonize=config.use_canonize,
        filename=config.filename,
        cutoff=config.cutoff,
        max_neighbors=config.max_neighbors,
        target_multiplication_factor=config.target_multiplication_factor,
        standard_scalar_and_pca=config.standard_scalar_and_pca,
        keep_data_order=config.keep_data_order,
        output_dir=config.output_dir
    )

    print(f"\næ•°æ®é›†å¤§å°:")
    print(f"  è®­ç»ƒé›†: {len(train_loader.dataset)}")
    print(f"  éªŒè¯é›†: {len(val_loader.dataset)}")
    print(f"  æµ‹è¯•é›†: {len(test_loader.dataset)}")
    print()

    # å¼€å§‹è®­ç»ƒ
    print("="*80)
    print("å¼€å§‹è®­ç»ƒ...")
    print("="*80 + "\n")

    start_time = time.time()

    train_dgl(
        config=config,
        train_val_test_loaders=[train_loader, val_loader, test_loader, prepare_batch],
        resume=args.resume,
        model_config=model_config  # Pass model config for checkpoint saving
    )

    end_time = time.time()
    elapsed_time = end_time - start_time

    print("\n" + "="*80)
    print(f"è®­ç»ƒå®Œæˆï¼")
    print(f"æ€»ç”¨æ—¶: {elapsed_time/3600:.2f} å°æ—¶ ({elapsed_time/60:.2f} åˆ†é’Ÿ)")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("="*80 + "\n")


# ==================== å¿«æ·è®­ç»ƒå‡½æ•° ====================

def train_jarvis_formation_energy():
    """JARVIS å½¢æˆèƒ½è®­ç»ƒçš„å¿«æ·å‡½æ•°"""
    sys.argv = [
        'train_with_cross_modal_attention.py',
        '--dataset', 'jarvis',
        '--property', 'formation_energy',
        '--use_cross_modal', 'True',
        '--cross_modal_num_heads', '4',
        '--epochs', '1000',
        '--batch_size', '64'
    ]
    main()


def train_mp_bandgap():
    """Material Project å¸¦éš™è®­ç»ƒçš„å¿«æ·å‡½æ•°"""
    sys.argv = [
        'train_with_cross_modal_attention.py',
        '--dataset', 'mp',
        '--property', 'band_gap',
        '--use_cross_modal', 'True',
        '--cross_modal_num_heads', '8',
        '--n_train', '60000',
        '--n_val', '5000',
        '--n_test', '4132',
        '--epochs', '1000',
        '--batch_size', '64'
    ]
    main()


def train_without_cross_modal():
    """ä¸ä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›è®­ç»ƒï¼ˆå¯¹æ¯”å®éªŒï¼‰"""
    sys.argv = [
        'train_with_cross_modal_attention.py',
        '--dataset', 'jarvis',
        '--property', 'formation_energy',
        '--use_cross_modal', 'False',  # ç¦ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›
        '--epochs', '1000',
        '--batch_size', '64'
    ]
    main()


# ==================== å…¥å£ ====================

if __name__ == "__main__":
    main()
